{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\junior_team\\miniconda3\\envs\\laser\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import CTransformers\n",
    "import sys\n",
    "import datetime\n",
    "from embed import Embeddor\n",
    "# from awq import AutoAWQForCausalLM\n",
    "# from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\Junior23\\Datasets\\MIT Lectures\\MIT_courses_chapters\\Introduction to Computer Science and Programming in Python_chapters\\\\video_1\\\\full_transcript.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    transcript = file.read()\n",
    "\n",
    "user_input = transcript #input(\"Enter your text: \")\n",
    "\n",
    "with open(\"user_input.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(user_input)\n",
    "\n",
    "\n",
    "loader = TextLoader(\"user_input.txt\")\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                             chunk_overlap=20)\n",
    "text_chunks=text_splitter.split_documents(documents)\n",
    "\n",
    "list_of_sentences = [text_chunks[x].page_content for x in range(len(text_chunks))]\n",
    "\n",
    "question=\"Summarize the transcript in a detailed way.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Summarize the transcript in a detailed way.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "(Score: 86.6734)\n",
      "(Score: 86.4912)\n",
      "(Score: 86.1197)\n",
      "(Score: 85.4176)\n",
      "(Score: 85.3636)\n",
      "[Document(page_content=\"modular, easy to understand. And not only that, not\\nonly will your code be read by other people,\\nbut next year, maybe, you'll take another\\ncourse, and you'll want to look back at\\nsome of the problems that you wrote in this class. You want to be able\\nto reread your code. If it's a big mess, you might\\nnot be able to understand-- or reunderstand--\\nwhat you were doing. So writing readable\\ncode and organizing code is also a big part. And the last section is going\", metadata={'source': 'user_input.txt'}), Document(page_content=\"flow to programs. That's what the second\\nline is going to be about. The second big\\npart of this course is a little bit more\\nabstract, and it deals with how do you write\\ngood code, good style, code that's readable. When you write code, you\\nwant to write it such that-- you're in big company,\\nother people will read it, other people will\\nuse it, so it has to be readable and\\nunderstandable by others. To that end, you\\nneed to write code that's well organized,\", metadata={'source': 'user_input.txt'}), Document(page_content=\"three different sections. The first one is related to\\nthese first two items here. It's really about\\nlearning how to program. Learning how to\\nprogram, part of it is figuring out what\\nobjects to create. You'll learn about these later. How do you represent knowledge\\nwith data structures? That's sort of the\\nbroad term for that. And then, as you're\\nwriting programs, you need to-- programs\\naren't just linear. Sometimes programs jump around. They make decisions. There's some control\", metadata={'source': 'user_input.txt'}), Document(page_content='addition, subtraction, and so on. What the memory contains\\nis a bunch of data and your sequence\\nof instructions. Interacting with the Arithmetic\\nLogic Unit is the Control Unit. And the Control Unit\\ncontains one program counter. When you load a sequence\\nof instructions, the program counter starts\\nat the first sequence. It starts at the sequence,\\nat the first instruction. It gets what the instruction\\nis, and it sends it to the ALU. The ALU asks, what are we', metadata={'source': 'user_input.txt'}), Document(page_content=\"I'm not stopping here. I'm going to keep going. If it's not close\\nenough, then I'm going to make a new guess\\nby averaging g and x over g. That's x over g here. And that's the\\naverage over there. And the new average is\\ngoing to be my new guess. And that's what it says. And then, the last step\\nis using the new guess, repeat the process. Then we go back to the beginning\\nand repeat the whole process over and over again. And that's what the\\nrest of the rows do. And you keep doing\", metadata={'source': 'user_input.txt'})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x0000018888B639A0>, search_kwargs={'k': 2})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = Embeddor()\n",
    "res, vector_embedding = embedder.get_top_k_from_texts(question, list_of_sentences)\n",
    "# print([text_chunks[i] for i in res])\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/msmarco-distilbert-base-tas-b', model_kwargs={'device':'cpu'})\n",
    "\n",
    "results_sentences = [text_chunks[i] for i in res]\n",
    "print(results_sentences)\n",
    "vector_store=FAISS.from_documents(text_chunks, embeddings)\n",
    "\n",
    "vector_store.as_retriever(search_kwargs={'k': 2})\n",
    "# print(vector_store.as_retriever(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "[INST] <<SYS>>\n",
    "Use the following pieces of information to answer the user's question. If you don't know the answer just say you don't know, don't try to make up an answer.\n",
    "<</SYS>>\n",
    "Context:{context}\n",
    "Question:{question}[/INST]\n",
    "Only return the helpful answer below and nothing else\n",
    "Helpful answer\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt=PromptTemplate(template=template, input_variables=['context', 'question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model downloaded\n"
     ]
    }
   ],
   "source": [
    "llm=CTransformers(model=\"QA_models/llama-2-7b-chat.gguf.q4_K_M.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens': 100, 'temperature':0,'context_length':512, 'gpu_layers': 10})\n",
    "print(\"model downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user is asking about how to compare programs in Python, specifically how to determine which program is better. The transcript provides information on three different sections related to learning how to program in Python:\n",
      "\n",
      "1. Representing knowledge with data structures: This section focuses on understanding how to use data structures to represent knowledge in a program.\n",
      "2. Learning how to program: This section covers the basics of programming, including creating objects and writing programs that are not linear.\n",
      "3. Compar\n"
     ]
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                   chain_type='stuff',\n",
    "                                   retriever=vector_store.as_retriever(search_kwargs={'k': 2}),\n",
    "                                   return_source_documents=True,\n",
    "                                   chain_type_kwargs={'prompt': qa_prompt})\n",
    "\n",
    "result=chain({'query':question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model downloaded\n"
     ]
    }
   ],
   "source": [
    "llm=CTransformers(model=\"QA_models/openinstruct-mistral-7b.Q4_K_M.gguf\",\n",
    "                  model_type=\"mistral\",\n",
    "                  config={'max_new_tokens': 100, 'temperature':0,'context_length':512, 'gpu_layers': 10})\n",
    "print(\"model downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two are actually part of the programming in Introduction to Programming and Computer Science in Python. And the last one deals mostly with the computer science part in Introduction to Programming and Computer Science in Python. We're going to talk about, once you have learned how to write programs in Python, how do you compare programs in Python? How do you know that one program is better than the other? How do you know\n",
      "\n",
      "three different sections. The first one is related to these first\n"
     ]
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                   chain_type='stuff',\n",
    "                                   retriever=vector_store.as_retriever(search_kwargs={'k': 2}),\n",
    "                                   return_source_documents=True,\n",
    "                                   chain_type_kwargs={'prompt': qa_prompt})\n",
    "\n",
    "result=chain({'query':question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ctranslate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"modular, easy to understand. And not only that, not\\nonly will your code be read by other people,\\nbut next year, maybe, you'll take another\\ncourse, and you'll want to look back at\\nsome of the problems that you wrote in this class. You want to be able\\nto reread your code. If it's a big mess, you might\\nnot be able to understand-- or reunderstand--\\nwhat you were doing. So writing readable\\ncode and organizing code is also a big part. And the last section is going flow to programs. That's what the second\\nline is going to be about. The second big\\npart of this course is a little bit more\\nabstract, and it deals with how do you write\\ngood code, good style, code that's readable. When you write code, you\\nwant to write it such that-- you're in big company,\\nother people will read it, other people will\\nuse it, so it has to be readable and\\nunderstandable by others. To that end, you\\nneed to write code that's well organized, three different sections. The first one is related to\\nthese first two items here. It's really about\\nlearning how to program. Learning how to\\nprogram, part of it is figuring out what\\nobjects to create. You'll learn about these later. How do you represent knowledge\\nwith data structures? That's sort of the\\nbroad term for that. And then, as you're\\nwriting programs, you need to-- programs\\naren't just linear. Sometimes programs jump around. They make decisions. There's some control addition, subtraction, and so on. What the memory contains\\nis a bunch of data and your sequence\\nof instructions. Interacting with the Arithmetic\\nLogic Unit is the Control Unit. And the Control Unit\\ncontains one program counter. When you load a sequence\\nof instructions, the program counter starts\\nat the first sequence. It starts at the sequence,\\nat the first instruction. It gets what the instruction\\nis, and it sends it to the ALU. The ALU asks, what are we I'm not stopping here. I'm going to keep going. If it's not close\\nenough, then I'm going to make a new guess\\nby averaging g and x over g. That's x over g here. And that's the\\naverage over there. And the new average is\\ngoing to be my new guess. And that's what it says. And then, the last step\\nis using the new guess, repeat the process. Then we go back to the beginning\\nand repeat the whole process over and over again. And that's what the\\nrest of the rows do. And you keep doing\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt = ' '.join([r.page_content for r in results_sentences])\n",
    "input_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "Use the following pieces of information to answer the user's question. If you don't know the answer just say you don't know, don't try to make up an answer.\n",
    "<</SYS>>\n",
    "Context:{input_prompt}\n",
    "Question:{question}[/INST]\n",
    "Only return the helpful answer below and nothing else\n",
    "Helpful answer\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContext:{input_prompt}\n",
    "Question:{question}\\n\\n### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'E:/cache/transformers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\junior_team\\miniconda3\\envs\\laser\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 752kB/s]\n",
      "Downloading config.json: 100%|██████████| 811/811 [00:00<00:00, 812kB/s]\n",
      "c:\\Users\\junior_team\\miniconda3\\envs\\laser\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\cache\\transformers. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 4.07MB/s]\n",
      "Downloading vocabulary.txt: 100%|██████████| 243k/243k [00:00<00:00, 571kB/s]\n",
      "Downloading model.bin:  28%|██▊       | 1.90G/6.74G [03:36<08:43, 9.27MB/s]"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "model_name = \"michaelfeil/ct2fast-open-llama-7b-open-instruct\"\n",
    "from transformers import AutoTokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"michaelfeil/ct2fast-open-llama-7b-open-instruct\", use_fast=False)\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",use_fast=False)\n",
    "from hf_hub_ctranslate2 import GeneratorCT2fromHfHub\n",
    "model = GeneratorCT2fromHfHub(\n",
    "        # load in int8 on CUDA\n",
    "        model_name_or_path=model_name,\n",
    "        device=\"cpu\",\n",
    "        compute_type='int8', #\"int8_float16\",\n",
    "        hub_kwargs={'cache_dir': 'E:/cache/transformers'},\n",
    "        tokenizer=tokenizer\n",
    "        # tokenizer=AutoTokenizer.from_pretrained(\"{ORG}/{NAME}\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    text=[\"Give me a story in which a person is trying to find a job.\"],\n",
    "    max_length=64,\n",
    "    include_prompt_in_result=False\n",
    ")\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Mistral-7B-v0.1-AWQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
    "                                          trust_remote_code=False, safetensors=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
    "\n",
    "# prompt = \"Tell me about AI\"\n",
    "# prompt_template=f'''{prompt}\n",
    "\n",
    "# '''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(\n",
    "    qa_prompt,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
